# 多臂老虎机算法

## 随机多臂老虎机

### 问题定义

!!! question "多臂老虎机问题"
    一个赌鬼要玩多臂老虎机，摆在他面前有 $K$ 个臂（Arms）或动作选择（Actions），每一轮游戏中，他要选择拉动一个臂并会获得一个随机奖励（reward）（这一随机奖励来源于一个赌场设定好的分布，但赌鬼一开始不知道这一分布）. 如果总共玩 $T$ 轮，他该如何最大化奖励？

多臂老虎机问题的本质：权衡探索（explore）与利用（exploit）.

根据用户操作时获得的反馈的不同，可以将老虎机问题分为三类：

- 完全反馈（full feedback）：能看见所有臂的奖励，例如投资组合中所有股票的涨跌；
- 部分反馈（partial feedback）：能看见部分臂的奖励，例如动态定价中任何更低（高）价格都被接受（拒绝）；
- 老虎机反馈（bandit feedback）：只能看见选择的臂的奖励，例如新闻网站上该新闻是否被用户点击

!!! definition "随机多臂老虎机问题模型"
    在每一步 $t = 1, 2, \ldots, T$ 中：

    1. 玩家选择一个臂 $a_t \in A = \{a_1, a_2, \ldots, a_K\}$；
    2. 玩家获得该臂对应的随机奖励 $r_t \sim R(a_t)$，其中 $r_t \in [0, 1]$；
    3. 玩家根据过往轮次的奖励情况调整选择的策略，实现奖励最大化.

    Notations:

    - $\mu(a_k) = \mathbb{E}[R(a_k)]$：臂 $a_k$ 奖励分布的均值；
    - $\mu^* = \max_{a \in A} \mu(a)$：最优臂 $a^*$ 的奖励均值.

可以看到随机多臂老虎机问题是一个最优化问题，我们的目标是最大化 $\sum\limits_{t=1}^T r_t$. 然而 $r_t$ 是一个随机变量，我们只能通过分析其均值来间接地最大化这个期望值.

从问题模型中我们可以看到，最优情况显然是每次都选择最优臂 $a^*$，但我们事先并不知道最优臂是哪个，一旦我们选择了非最优臂 $a_t$，就会导致一部分的奖励损失. 我们使用遗憾（regret）来衡量这些损失：

!!! definition "遗憾"
    1. 伪遗憾（pseudo-regret）：

        $$
            R(T) = \sum_{t = 1}^T (\mu^* - \mu(a_t)) = \mu^* \cdot T - \sum_{t = 1}^T \mu(a_t).
        $$

    2. 期望遗憾（expected regret）：$\mathbb{E}[R(T)]$.

于是奖励最大化问题便转化为了遗憾最小化问题. 我们通常使用遗憾界（regret bound）来衡量某个多臂老虎机算法的优劣，一个好的遗憾算法需要具有次线性（sub-linear）的遗憾界.

### 贪心算法

!!! theorem "Hoeffding Inequality"
    假设 $X_1, X_2, \ldots, X_n$ 是 $[0, 1]$ 上的独立随机变量，样本均值 $\overline{X}_n = \dfrac{1}{n} \sum\limits_{i=1}^n X_i$，$\mu = \mathbb{E}[\overline{X}_n]$，则对于任意 $\varepsilon > 0$，有

    $$
        \mathbb{P}(|\mu - \overline{X}_n| \geqslant \varepsilon) \leqslant 2 e^{-2n \varepsilon^2}.
    $$

贪心算法的思路：

- 探索阶段：将所有臂都尝试 $N$ 遍
- 利用阶段：始终选择当前估计奖励最高的臂，并利用历史经验更新对每个臂的奖励估计值.

    - 选择：$\hat{a} = \mathrm{argmax}_a Q_t(a)$
    - 更新：$N_{t+1}(\hat{a}) = N_t(\hat{a}) + 1, \enspace Q_{t+1}(a) = Q_t(a) + \dfrac{r_t - Q_t(\hat{a})}{N_{t+1}(\hat{a})}$.

遗憾界：$O(T^{2/3} (K \log T)^{1/3})$.

### $\varepsilon$-贪心算法

对普通贪心算法的改进：以 $1 - \varepsilon$ 的概率选择当前已在最优的臂 $a' = \mathrm{argmax}_a Q_t(a)$，以 $\varepsilon$ 的概率随机选择一个臂. 通过调整 $\varepsilon$ 的值，可以控制探索和利用之间的平衡.

令 $\epsilon_t = t^{-1/3} (K \log t)^{1/3}$，则 $\varepsilon$-贪心算法的遗憾界为 $O(T^{2/3} (K \log T)^{1/3})$.



### 上置信界算法

与贪心算法相比，上置信界算法的选择策略发生了变化：

$$
    a_t = \mathrm{argmax}_a \left(Q_t(a) + \sqrt{\frac{2 \ln t}{N_t(a)}} \right)
$$

即根据每个臂的奖励的置信区间上界来选择臂. 可以看到，被选择更多次的臂的 $\sqrt{\frac{2 \ln t}{N_t(a)}}$ 更小，表明随着选择次数的增多，均值的不确定性会减小. 因此上置信界算法会鼓励玩家尝试较少被选择的臂，避免陷入局部最优解.

遗憾界：$O(\sqrt{KT \log T})$.

### 汤普森采样算法

基本思路：采用贝叶斯方法，为每个臂维护一个先验概率分布，表示对该臂奖励概率的信念. 每次选择臂时，从每个臂的后验概率分布中进行采样，并选择采样值最高的臂。最后，根据获得的奖励更新所选臂的后验概率分布，从而在探索和利用之间取得平衡.

我们可以选取任意共轭先验分布（先验分布和后验分布输入同一个分布族）作为每个臂的选择概率分布，例如 Beta 分布 $B(\alpha, \beta)$，其中 $\alpha$ 和 $\beta$ 分别表示伯努利试验中的成功和失败次数.

每轮迭代中，我们先对每个臂进行采样：

$$
    \theta_t(a_k) \sim B(S_t(a_k) + 1, F_t(a_k) + 1)
$$

然后选择具有最大 $\theta_t(a)$ 的臂 $a_t$：$a_t = \mathrm{argmax}_a \theta_t(a)$，并根据奖励 $r_t \in \{0, 1\}$ 更新后验概率分布：

$$
    S_{t+1}(a_t) = S_t(a_t) + r_t, \enspace
    F_{t+1}(a_t) = F_t(a_t) + (1 - r_t).
$$

若 $r_t \in [0, 1]$，则以 $r_t$ 为概率从 $\{0, 1\}$ 中抽一个数作为反馈.

遗憾界：$O(\sqrt{KT \log T})$.


## 对抗性多臂老虎机

!!! definition "对抗性多臂老虎机问题模型"
    在每一步 $t = 1, 2, \ldots, T$ 中：

    1. 玩家选择一个行动集合 $[n] = \{1, 2, \ldots, n\}$ 上的概率分布 $p_t$；
    2. 对手在已知 $p_t$ 的情况下选择一个代价向量 $c_t \in [0, 1]^n$；
    3. 玩家根据概率分布 $p_t$ 选择一个行动 $i_t \sim p_t$，并得到代价 $c_t(i_t)$；
    4. 玩家通过 $c_t(i_t)$ 调整未来的策略.

    玩家的目标：选取一个策略序列 $p_1, p_2, \ldots, p_T$，使得总代价最小，即最小化

    $$
        \mathbb{E}_{i_t \sim p_t}\left[ \sum_{t=1}^T c_t(i_t) \right].
    $$

我们同样需要定义遗憾. 为了能使用遗憾界来分析算法的优劣，我们希望遗憾在最优情况下应该能取到 0.

=== "定义 1"

    与所有轮次结束后的事后最优作差比较：

    $$
        R_T = \mathbb{E}_{i_t \sim p_t} \left[ \sum_{t=1}^T (c_t(i_t) - \min_{i \in [n]} c_t(i)) \right].
    $$

    而在事先知晓代价向量的情况下，最优算法的期望代价为 0，但这是不可能做到的.

=== "定义 2"

    与每轮选取一个固定的代价向量能得到的最优解作差比较：

    $$
        R_T = \mathbb{E}_{i_t \sim p_t} \left[ \sum_{t=1}^T c_t(i_t) \right] - \min_{i \in [n]} \sum_{t=1}^T c_t(i).
    $$

上述两种定义都可以描述遗憾，但第一种定义可能导致最优情况下遗憾仍然是线性级别的，而第二种定义则可以保证最优情况下遗憾为 0.

!!! definition "no-regret"
    若当 $T \to \infty$ 时，平均遗憾 $\dfrac{\overline{R}_T}{T} \to 0$（即 $R_T$ 关于 $T$ 是次线性的），则称算法为无憾算法.

第二种定义的合理性在于，有自然的算法实现无憾，但无憾的实现也不是平凡的（算法之间有区分度）. 下面我们的目标便是设计一个无憾的在线学习算法.

### 无憾算法设计

一个简单的在线学习算法是跟风算法（Follow-The-Leader, FTL）：

!!! definition "跟风算法"
    在每一个时间点 $t$，选择最小累积代价的行动 $i$，即

    $$
        i = \mathrm{argmin}_{i \in [n]} \sum_{s=1}^{t-1} c_s(i).
    $$

可以构造反例证明跟风算法不是无憾算法.

跟风算法是一种确定性算法，即每轮中玩家给出的策略 $p_t$ 都是 0-1 向量. 与之对应的，随机化算法是指玩家在每一轮中给出的策略 $p_t$ 不一定是 0-1 向量的算法.

事实上，任意确定性算法都会存在线性级别的遗憾 $\dfrac{n-1}{n}T$，其中 $n$ 是可能行动的个数. 其原因是对手知道我们的策略中哪个行为的概率为 1，这样他就只需要设置我们选择的行为有代价 1，其余行为代价为 0 即可. 如果我们采用随机化算法，则对手无法通过 $p_t$ 得知我们的具体行动.

!!! theorem "乘性权重算法"
    乘性权重算法（Multiplicative Weights Algorithm, MWA）是一种随机化算法，其策略是：

    - 初始化权重向量 $\mathbf{w}_1 = (1, 1, \ldots, 1)$；
    - 在每轮迭代中
        - 计算 $W_t = \sum_{i \in [n]} w_t(i)$，并设置 $\mathbf{p}_t = \dfrac{\mathbf{w}_t}{W_t}$；
        - 选取 $i_t \sim \mathbf{p}_t$；
        - 对手给出代价向量 $\mathbf{c}_t \in [0, 1]^n$；
        - 根据代价向量更新权重向量：对于任意 $i \in [n]$，有

            $$
                \mathbf{w}_{t+1}(i) = \mathbf{w}_{t}(i) \cdot (1 - \epsilon \cdot \mathbf{c}_{t}(i)).
            $$

乘性权重算法的遗憾界为 $O(\sqrt{T \ln n})$，因而是无憾的.


### 多臂老虎机的应用

!!! question "数据定价问题"
    假设你要出售一份数据，你知道会有 $N$ 个人来购买你的数据，并且每个人对数据的估值 $v$ 都完全一致，都在 $[0, 1]$ 中。买家是逐个到达的，你需要提供一个价格 $p$，如果 $v \geqslant p$，买家就会购买你的数据，否则买家会离开。你的目标是尽快地学习到 $v$ 的值，误差范围是 $\varepsilon = \dfrac{1}{N}$.

- 简单的想法：二分搜索

    由于误差范围为 $1 / N$，因而最多 $\log N$ 次搜索之后可以达到这一精度；如果仍然无法达到，就取 $p \geqslant \widetilde{v} - \varepsilon$ 作为价格.

    在这种情况下，$N$ 轮之后的总收益为

    $$
        (\log N) \cdot 0 + (N - \log N) \left( v - \frac{2}{N} \right) \approx vN - v\log N - 2.
    $$

    故遗憾 $R \approx v\log N + 2$.

- 改进算法

    ![pVKY8wd.md.png](https://s21.ax1x.com/2025/07/04/pVKY8wd.md.png)

    改进算法的遗憾至多为 $1 + 2 \log \log N$.
